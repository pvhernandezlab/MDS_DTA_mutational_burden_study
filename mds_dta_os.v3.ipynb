{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecd5bb5",
   "metadata": {},
   "source": [
    "# A PROGNOSTIC MODEL FOR MYELODYSPLASTIC SYNDROMES BASED ON DTA MUTATIONAL BURDEN: DEVELOPMENT AND VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840310af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# GLOBAL WARNING CONTROL\n",
    "# ------------------------------------------------------------------\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea4ae0",
   "metadata": {},
   "source": [
    "# Basic packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd811db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import i2bmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e0d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9084cfc",
   "metadata": {},
   "source": [
    "# Overall survival (OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d6dbe",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b49f7",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f67bd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Path definitions\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder where this script lives \n",
    "HERE = Path.cwd()\n",
    "\n",
    "# Project root -> go one level up from HERE\n",
    "PROJECT_ROOT = HERE.parent\n",
    "\n",
    "# Get paths to the data output folders\n",
    "OUTPUT = PROJECT_ROOT / \"output\"\n",
    "FIGURES = PROJECT_ROOT / \"figures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data using saved csv file (created based on data pre-processing script)\n",
    "df=pd.read_csv(OUTPUT / \"mds_dta_cohort_os.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37729b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data in features and outcomes (X and y)\n",
    "target_cols = ['os_months', 'os_status']\n",
    "X = df.drop(columns=target_cols)\n",
    "y = df[['os_months', 'os_status']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6d7d0",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Testing Sets\n",
    "\n",
    "The most important step to avoid Data Leakage, carried out using the `train_test_split` function from the `sklearn.model_selection` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40481ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y['os_status']) #to ensure fair distribution of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a128ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f9bfa",
   "metadata": {},
   "source": [
    "# Standardization/ Data Scaler (z-score)\n",
    "\n",
    "It centers features to mean 0 and standard deviation as 1. Since most ML algorithms assume approx normal distribution, scaling is used to improve performance, particularly in classification models. \\\n",
    "Once training data is established, we pursued scaling using `StandardScaler`.\n",
    "Scaling was performed before imputation since we are using MICE, which can be affected by magnitude of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the Scaler and chose columns to scale (only float64 columns need to normalize and few extra ones, do not normalize dummies even if \"numerical\")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit the scaler *only* on the training data\n",
    "scaler = StandardScaler()\n",
    "cols_to_scale = X.select_dtypes(include=['float64']).columns.tolist()\n",
    "extra_cols = ['asxl1_only_counts', 'dta_non_asxl1_counts', 'asxl1_mixed_counts', 'dta_non_asxl1_counts', 'truncating_variant', 'asxl1_truncating_variant', 'dnmt3a_truncating_variant', 'tet2_truncating_variant', 'n_truncating_variant', 'pathogenic_asxl1', 'pathogenic_dnmt3a', 'pathogenic_tet2']\n",
    "\n",
    "all_cols_to_scale = cols_to_scale + extra_cols\n",
    "\n",
    "# Fit on all columns\n",
    "scaler.fit(X_train[all_cols_to_scale])\n",
    "\n",
    "# Transform all columns\n",
    "X_train[all_cols_to_scale] = scaler.transform(X_train[all_cols_to_scale])\n",
    "X_test[all_cols_to_scale] = scaler.transform(X_test[all_cols_to_scale]) #trained on train, applied to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72b2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c35cd",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "The dataset has missing values (NaN). To manage this (and since the data has been already split to avoid data leakage):\n",
    "1. CADD and phyloP (MOST LIKELY) will have missing values if truncating_variant = 1 since they are novel frameshifts not reported in databases (ClinVar and gnomAD). \n",
    "2. That said, we will performed imputation using MICE appproach and Random Forest Regressor as the estimator. \n",
    "3. Finally, the very fist step was add missingness indicators for every predictor with `NaN` (already done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MICE imputation for remaining missing values (Train set only)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Identifying columns with missing values (tolist)\n",
    "cols_with_missing = X_train.columns[X_train.isnull().any()].tolist()\n",
    "\n",
    "# Use all columns as predictors\n",
    "predictor_cols = X_train.columns.tolist()\n",
    "\n",
    "# Initialize MICE imputer with Random Forest as estimator\n",
    "mice_imputer = IterativeImputer(\n",
    "    random_state=42,\n",
    "    max_iter=10,\n",
    "    estimator=RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    sample_posterior=False\n",
    ")\n",
    "\n",
    "# Fit MICE imputer on training data (it will return as array)\n",
    "data_imputed_array_train = mice_imputer.fit_transform(X_train[predictor_cols])\n",
    "\n",
    "# Convert array back to DataFrame (full imputed train matrix)\n",
    "X_train_imputed_full = pd.DataFrame(\n",
    "    data_imputed_array_train,\n",
    "    columns=predictor_cols,\n",
    "    index=X_train.index\n",
    ")\n",
    "\n",
    "# Overwrite ONLY columns that had missing values\n",
    "X_train[cols_with_missing] = X_train_imputed_full[cols_with_missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MICE imputation for testing set\n",
    "\n",
    "# Apply MICE imputer on testing data (using training-fitted imputer)\n",
    "data_imputed_array_test = mice_imputer.transform(X_test[predictor_cols])\n",
    "\n",
    "# Convert output array back to DataFrame (full imputed test matrix)\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    data_imputed_array_test,\n",
    "    columns=predictor_cols,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Overwrite ONLY columns that had missing values\n",
    "X_test[cols_with_missing] = X_test_imputed[cols_with_missing]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1cc794",
   "metadata": {},
   "source": [
    "# Machine Learning Models\n",
    "Considering we are evaluating survival anlaysis as outcomes, we searched for related models. \n",
    "1. **CoxPH** is a traditional statistical analysis that struggles with high-dimensional data and multicollinearity. However `CoxnetSurvivalAnalysis` is a machine learning model that predicts survival in high-dimensional data settings. It is less vulnerable to overfitting due to Elastic Net (L1 and L2) penalties (PMID: 37884606, 27065756)\n",
    "\n",
    "2. **Random Survival Forest** is a nonparametric machine learning model that handles non-linear interactions. Other similar studies have used this model (see abstract)\n",
    "\n",
    "3. **Gradient-Boosted Survival Trees**: \"A gradient boosted model is similar to a Random Survival Forest, in the sense that it relies on multiple base learners to produce an overall prediction, but differs in how those are combined. While a Random Survival Forest fits a set of Survival Trees independently and then averages their predictions, a gradient boosted model is constructed sequentially in a greedy stagewise fashion.\" (scikit-survival)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca568be",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b826a7f0",
   "metadata": {},
   "source": [
    "Hyperparameter tuning (or optimization) is the process of finding the best set of hyperparameter values that maximize model performance (e.g., accuracy, AUC, F1, etc.) on validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e82133",
   "metadata": {},
   "source": [
    "### Note: \n",
    "We have experienced issue with censoring (time_train.max), since every time point that is passed must lie in a region where the censoring Kaplan-Meier (under the hood analysis) is defined (i.e., up to the last censoring time in the training set) AND must have follow-up in the test set (i.e., between the minimun and maximum observed test times). U;timately this was caused by `integrated_brier_score`. We fixed having the IBS evaluation to always construct a safe time grid from the durrent data instead of hard-coding percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare IBS evaluation function and time grid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def prepare_ibs_evaluation(y_train_surv, y_test_surv, X_test, n_grid=200, eps=1e-3):\n",
    "    \"\"\"\n",
    "    Prepare a test subset and time grid that are valid for integrated_brier_score.\n",
    "\n",
    "    IBS restriction:\n",
    "      - Only test patients with follow-up < max(train time) are used.\n",
    "      - Time grid is chosen within the overlap of train times and IBS-eligible test times.\n",
    "    \"\"\"\n",
    "    time_train = y_train_surv[\"os_months\"]\n",
    "    time_test_all = y_test_surv[\"os_months\"]\n",
    "\n",
    "    # Max observed time in TRAIN (this is what sksurv's censoring KM uses)\n",
    "    max_train_time = time_train.max()\n",
    "\n",
    "    # Keep only test patients with follow-up strictly below max_train_time\n",
    "    ibs_mask = time_test_all < max_train_time\n",
    "    n_ibs = ibs_mask.sum()\n",
    "    if n_ibs < 2:\n",
    "        raise ValueError(f\"Too few IBS-eligible test patients: {n_ibs}.\")\n",
    "\n",
    "    # Structured array subset for Surv\n",
    "    y_test_surv_ibs = y_test_surv[ibs_mask]\n",
    "\n",
    "    # Match X_test rows\n",
    "    if hasattr(X_test, \"iloc\"):\n",
    "        idx = np.where(ibs_mask)[0]\n",
    "        X_test_ibs = X_test.iloc[idx]\n",
    "    else:\n",
    "        X_test_ibs = X_test[ibs_mask]\n",
    "\n",
    "    # Build a time grid in the overlap between train and IBS-eligible test\n",
    "    time_test_ibs = y_test_surv_ibs[\"os_months\"]\n",
    "\n",
    "    t_min = max(time_train.min(), time_test_ibs.min())\n",
    "    t_max = min(max_train_time, time_test_ibs.max()) - eps  # stay strictly below max_train_time\n",
    "\n",
    "    if t_max <= t_min:\n",
    "        raise ValueError(f\"Invalid IBS time window: t_min={t_min}, t_max={t_max}\")\n",
    "\n",
    "    time_grid = np.linspace(t_min, t_max, n_grid, endpoint=False)\n",
    "\n",
    "    return X_test_ibs, y_test_surv_ibs, time_grid, max_train_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d53ae9",
   "metadata": {},
   "source": [
    "### CoxNetSurvival\n",
    "We are doing simple `GridSearch` as there are only two parameters (alpha and L1 regularization). That said, it is feasible to search for all possibilities/combinations \\\n",
    "To evaluate performance, we are using Harrell's C-index and Integrated Brier Score (IBS), \n",
    "\n",
    "References: \n",
    "1. Haider H., Hoehn B eet al. Effective  Ways to Build and Evaluate Individual Survival Distributions. Journal of Machine Learning Research 21 (2020) 1-63 \n",
    "2. Ping Wang, Yan Li, and Chandan k. Reddy. 2019. Machine Learning for Survival Analysis: A Survey. ACM\n",
    "Comput. Surv. 51, 6, Article 110 (February 2019)\n",
    "3. https://scikit-survival.readthedocs.io/en/latest/user_guide/evaluating-survival-models.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9881e3",
   "metadata": {},
   "source": [
    "### Note:\n",
    "In the earlier analysis, all patients with zero survival duration were randomly assigned to the test set only, so `CoxnetSurvivalAnalysis` never encountered a non-positive time in model fitting. \\\n",
    "After feature engineering and column removal, the train/test distribution changed, placing some zero-time patients in the training folds. \\\n",
    "`CoxnetSurvivalAnalysis` requires strictly positive survival times and will crash (segfault) when encountering zero values during partial likelihood computation. \\\n",
    "Adjusting the time origin by adding a small constant (e.g., 0.01 months) prevents this issue without affecting hazard ratios or model discrimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd83034",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Avoid zero months by clipping at small value and avoid kernel crashes\n",
    "\n",
    "epsilon = 0.01\n",
    "\n",
    "y_train[\"os_months\"] = y_train[\"os_months\"].clip(lower=epsilon)\n",
    "y_test[\"os_months\"]  = y_test[\"os_months\"].clip(lower=epsilon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592cf451",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Some correlated features remained even after dropping fold-constant ones.\n",
    "`CoxnetSurvivalAnalysis` is extremely sensitive to correlation (weights blow up at small alpha and small tol (1e−7))\n",
    "\n",
    "So the solution was: increase regularization + relax the solver tolerance\n",
    "\n",
    "The `sksurv` authors explicitly recommend:\n",
    "- alpha_min ≥ 0.01 for medium-sized datasets\n",
    "- tol ≥ 1e−6\n",
    "- max_iter ≤ 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coxnet model with hyperparameter tuning, C-index and IBS evaluation\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sksurv.linear_model import CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "from sksurv.util import Surv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Custom scoring function - C-index for GridSearchCV\n",
    "def cindex_scorer(estimator, X, y_struct):\n",
    "    pred = estimator.predict(X)\n",
    "    return concordance_index_censored(\n",
    "        y_struct[\"os_status\"], y_struct[\"os_months\"], pred\n",
    "    )[0]\n",
    "\n",
    "# Model\n",
    "cox = CoxnetSurvivalAnalysis(\n",
    "    fit_baseline_model=True,  # needed for predict_survival_function\n",
    "    max_iter=20000, #max iterations or steps for convergence\n",
    "    tol=1e-6  # tolerance for convergence (min change in coefficients)\n",
    ")\n",
    "\n",
    "# Parameter grid\n",
    "alpha_path = np.logspace(-2, 1, 50)\n",
    "param_grid_coxnet = {\n",
    "    \"alphas\": [alpha_path], #intensity of regularization\n",
    "    \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9], #balance between L1 and L2 regularization\n",
    "}\n",
    "\n",
    "# Cross-validation setup (in the training set)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search setup\n",
    "grid_search_coxnet = GridSearchCV(\n",
    "    estimator=cox,\n",
    "    param_grid=param_grid_coxnet,\n",
    "    cv=cv,\n",
    "    scoring=cindex_scorer,\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    "    verbose=1,\n",
    "    error_score=\"raise\"\n",
    ")\n",
    "\n",
    "## Prepare survival data for training and test\n",
    "y_train = y_train.copy()\n",
    "y_train[\"os_status\"] = y_train[\"os_status\"].fillna(0).astype(int).astype(bool)\n",
    "\n",
    "y_test = y_test.copy()\n",
    "y_test[\"os_status\"] = y_test[\"os_status\"].fillna(0).astype(int).astype(bool)\n",
    "\n",
    "y_train_surv = Surv.from_dataframe(\"os_status\", \"os_months\", y_train)\n",
    "y_test_surv  = Surv.from_dataframe(\"os_status\", \"os_months\", y_test)\n",
    "\n",
    "# Fit model\n",
    "grid_search_coxnet.fit(X_train, y_train_surv)\n",
    "\n",
    "print(\"Best params:\", grid_search_coxnet.best_params_)\n",
    "print(f\"Best C-index on train (CV): {grid_search_coxnet.best_score_:.4f}\")\n",
    "\n",
    "cox_best = grid_search_coxnet.best_estimator_\n",
    "\n",
    "\n",
    "## Test metrics – C-index\n",
    "\n",
    "pred_test = cox_best.predict(X_test)\n",
    "c_index_test = concordance_index_censored(\n",
    "    y_test_surv[\"os_status\"],\n",
    "    y_test_surv[\"os_months\"],\n",
    "    pred_test\n",
    ")[0]\n",
    "print(f\"C-index on test set (full): {c_index_test:.4f}\")\n",
    "\n",
    "### IBS on IBS-eligible subset (subsampled, with safe grid)\n",
    "\n",
    "#epsilon = 1e-3  # small margin so times are strictly inside follow-up\n",
    "\n",
    "# IBS-eligible subset using helper\n",
    "X_test_cox_ibs, y_test_surv_ibs_cox, TIME_GRID_COX, max_train_time_cox = prepare_ibs_evaluation(\n",
    "    y_train_surv, y_test_surv, X_test, n_grid=100   # 100 time points is enough\n",
    ")\n",
    "\n",
    "print(f\"Max train time (Cox): {max_train_time_cox:.2f}\")\n",
    "print(f\"Max test time (all): {y_test_surv['os_months'].max():.2f}\")\n",
    "print(f\"Max IBS-eligible test time (Cox): {y_test_surv_ibs_cox['os_months'].max():.2f}\")\n",
    "print(f\"TIME_GRID_COX range (pre-subsample): {TIME_GRID_COX.min():.2f} to {TIME_GRID_COX.max():.2f}\")\n",
    "print(f\"IBS-eligible test size (before subsampling): {len(y_test_surv_ibs_cox)}\")\n",
    "\n",
    "# Subsample IBS test set for speed\n",
    "max_ibs_test = 300\n",
    "n_ibs_cox = len(y_test_surv_ibs_cox)\n",
    "\n",
    "if n_ibs_cox > max_ibs_test:\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx_sub_cox = rng.choice(n_ibs_cox, size=max_ibs_test, replace=False)\n",
    "\n",
    "    if hasattr(X_test_cox_ibs, \"iloc\"):\n",
    "        X_test_cox_ibs_sub = X_test_cox_ibs.iloc[idx_sub_cox]\n",
    "    else:\n",
    "        X_test_cox_ibs_sub = X_test_cox_ibs[idx_sub_cox]\n",
    "\n",
    "    y_test_surv_ibs_sub_cox = y_test_surv_ibs_cox[idx_sub_cox]\n",
    "    print(f\"Subsampled IBS test size (Cox): {len(y_test_surv_ibs_sub_cox)} (from {n_ibs_cox})\")\n",
    "else:\n",
    "    X_test_cox_ibs_sub = X_test_cox_ibs\n",
    "    y_test_surv_ibs_sub_cox = y_test_surv_ibs_cox\n",
    "\n",
    "# Recompute a VALID time grid for this subsample\n",
    "time_train = y_train_surv[\"os_months\"]\n",
    "time_test_sub_cox = y_test_surv_ibs_sub_cox[\"os_months\"]\n",
    "\n",
    "t_min = max(time_train.min(), time_test_sub_cox.min())\n",
    "t_max = min(time_train.max(), time_test_sub_cox.max()) - epsilon  # strictly < max(test_time)\n",
    "\n",
    "if t_max <= t_min:\n",
    "    raise ValueError(f\"Invalid IBS time window after subsample (Cox): t_min={t_min}, t_max={t_max}\")\n",
    "\n",
    "n_grid = 100\n",
    "TIME_GRID_COX_SUB = np.linspace(t_min, t_max, n_grid, endpoint=False)\n",
    "\n",
    "print(f\"TIME_GRID_COX_SUB range: {TIME_GRID_COX_SUB.min():.2f} to {TIME_GRID_COX_SUB.max():.2f}\")\n",
    "print(f\"Cox subsample follow-up range: {time_test_sub_cox.min():.2f} to {time_test_sub_cox.max():.2f}\")\n",
    "\n",
    "# Predict survival functions on the subsample\n",
    "t0 = time.time()\n",
    "surv_fns_cox = list(cox_best.predict_survival_function(X_test_cox_ibs_sub))\n",
    "print(f\"Cox predict_survival_function done in {time.time() - t0:.2f} s\")\n",
    "\n",
    "# Evaluate survival on the new grid\n",
    "t1 = time.time()\n",
    "surv_probs_cox = np.asarray([fn(TIME_GRID_COX_SUB) for fn in surv_fns_cox])\n",
    "print(f\"Building surv_probs_cox done in {time.time() - t1:.2f} s\")\n",
    "print(\"surv_probs_cox shape:\", surv_probs_cox.shape)\n",
    "\n",
    "# IBS\n",
    "t2 = time.time()\n",
    "ibs_test = integrated_brier_score(\n",
    "    y_train_surv,\n",
    "    y_test_surv_ibs_sub_cox,\n",
    "    surv_probs_cox,\n",
    "    TIME_GRID_COX_SUB\n",
    ")\n",
    "print(f\"integrated_brier_score (Cox) done in {time.time() - t2:.2f} s\")\n",
    "\n",
    "print(f\"IBS on test set (IBS-eligible subset, subsampled): {ibs_test:.4f}\")\n",
    "\n",
    "results = {\"c_index_test\": c_index_test, \"ibs_test\": ibs_test}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4313ae",
   "metadata": {},
   "source": [
    "## Bootstrap CoxSurvival\n",
    "\n",
    "We decided to run bootstrap to be consistent with similar studies and provide uncertainty in the C-index score. \\\n",
    "Boostrap: will pick `n_test` (nomber of row of test set) 1,000 times (B = 1,000). Some may be repeat, some may not appear since it is with replacement. Then, it will compute C-index and IBS for each iteration. From this, we will get percentiles 2.5 - 97.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bootstrap C-index (CoxNet)\n",
    "\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Point estimate on full test set\n",
    "time_test  = y_test_surv[\"os_months\"]\n",
    "event_test = y_test_surv[\"os_status\"].astype(bool)\n",
    "\n",
    "risk_test = cox_best.predict(X_test)\n",
    "\n",
    "cindex_test_cox = concordance_index_censored(\n",
    "    event_test,\n",
    "    time_test,\n",
    "    risk_test\n",
    ")[0]\n",
    "\n",
    "print(f\"C-index (test, CoxNet) point estimate: {cindex_test_cox:.4f}\")\n",
    "\n",
    "\n",
    "# Bootstrap 95% CI on full test set\n",
    "rng = np.random.default_rng(123)\n",
    "B = 1000\n",
    "\n",
    "n_test = X_test.shape[0]\n",
    "cindex_boot_cox = np.empty(B, dtype=float)\n",
    "\n",
    "print(f\"Bootstrap C-index on full test set ({n_test} patients) – CoxNet.\")\n",
    "\n",
    "for b in range(B):\n",
    "    idx = rng.integers(0, n_test, size=n_test)\n",
    "\n",
    "    Xb = X_test.iloc[idx]\n",
    "    tb = time_test[idx]\n",
    "    eb = event_test[idx]\n",
    "\n",
    "    risk_b = cox_best.predict(Xb)\n",
    "\n",
    "    cindex_b = concordance_index_censored(\n",
    "        eb,\n",
    "        tb,\n",
    "        risk_b\n",
    "    )[0]\n",
    "\n",
    "    cindex_boot_cox[b] = cindex_b\n",
    "\n",
    "cindex_ci_cox = np.percentile(cindex_boot_cox, [2.5, 97.5])\n",
    "\n",
    "print(\n",
    "    f\"C-index (test, CoxNet): {cindex_test_cox:.4f} | \"\n",
    "    f\"95% CI [{cindex_ci_cox[0]:.4f}, {cindex_ci_cox[1]:.4f}]\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942abdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bootstrap IBS on subsampled IBS-eligible set (Cox) --- no C-index in this code\n",
    "\n",
    "# Bootstrap setup\n",
    "rng = np.random.default_rng(123)  # reproducible bootstrap\n",
    "B = 1000                          # number of bootstrap resamples\n",
    "\n",
    "ibs_boot = []\n",
    "\n",
    "time_train  = y_train_surv[\"os_months\"]\n",
    "event_train = y_train_surv[\"os_status\"]\n",
    "\n",
    "censor_times = time_train[~event_train]\n",
    "if censor_times.size == 0:\n",
    "    raise ValueError(\"Cannot compute IBS/bootstrap: no censored observations in training data.\")\n",
    "max_train_time = time_train.max()\n",
    "eps = 1e-3\n",
    "\n",
    "n_ibs = X_test_cox_ibs_sub.shape[0]\n",
    "print(f\"Bootstrap IBS on {n_ibs} subsampled IBS-eligible test patients (Cox).\")\n",
    "\n",
    "for _ in range(B):\n",
    "    # Resample within subsampled IBS set\n",
    "    idx_ibs = rng.integers(0, n_ibs, size=n_ibs)\n",
    "\n",
    "    Xb_ibs = X_test_cox_ibs_sub.iloc[idx_ibs]\n",
    "    yb_surv = y_test_surv_ibs_sub_cox[idx_ibs]\n",
    "    yb_event = yb_surv[\"os_status\"]\n",
    "    yb_time  = yb_surv[\"os_months\"]\n",
    "\n",
    "    # Safe time window for this bootstrap replicate\n",
    "    t_min_b = max(time_train.min(), yb_time.min())\n",
    "    t_max_b = min(max_train_time, yb_time.max()) - eps\n",
    "\n",
    "    if t_max_b <= t_min_b:\n",
    "        continue\n",
    "\n",
    "    time_points_b = np.linspace(t_min_b, t_max_b, 100, endpoint=False)\n",
    "\n",
    "    surv_fns_b = list(cox_best.predict_survival_function(Xb_ibs))\n",
    "    surv_probs_b = np.asarray([fn(time_points_b) for fn in surv_fns_b])\n",
    "\n",
    "\n",
    "# C-index\n",
    "\n",
    "\n",
    "    ibs_b = integrated_brier_score(\n",
    "        y_train_surv,\n",
    "        yb_surv,\n",
    "        surv_probs_b,\n",
    "        time_points_b\n",
    "    )\n",
    "    ibs_boot.append(ibs_b)\n",
    "\n",
    "ibs_boot = np.array(ibs_boot)\n",
    "ibs_ci = np.percentile(ibs_boot, [2.5, 97.5])\n",
    "\n",
    "print(f\"IBS     (test, subsampled IBS subset): {ibs_test:.4f} | 95% CI [{ibs_ci[0]:.3f}, {ibs_ci[1]:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1f3cf",
   "metadata": {},
   "source": [
    "# Random Survival Forest\n",
    "We are implementing hyperparameter tuning by Bayesian optimization (Optuna) since this model will analyze high dimensional and nonlinear relationships, which is exhaustive using `GridSearch` and Random Search may leave the best parameter setiting out. Bayesian approach uses past results to model the parameter-performance relationship and pick the next promising set. Bayesian optimization builds a surrogate models that learns a mapping. Each trial is not random, as this approach chooses the next parameter set based on where it expects the biggest improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac655d",
   "metadata": {},
   "source": [
    "### Key features of `optuna`\n",
    "1. **Objective function**: The core of Optuna is the objective function, which encapsulates the model training and evaluation process. This function takes a trial object as an argument, allowing hyperparameter values for the current trial.\n",
    "2. **Trial Object**: The trial object within the objective function provides methods like `suggest_float()`, `suggest_int()`, `suggest_categorical(`), etc., to define the search space for different types of hyperparameters.\n",
    "3. **Study**: A `Study` object in Optuna manages the optimization process. You create a study, specify the optimization direction (maximize or minimize the objective function's return value), and then call `study.optimize()` with your objective function and the number of trials.\n",
    "4. **Pruners**: `optuna` includes pruners that can stop unpromising trials early based on intermediate evaluation scores, saving computational resources.\n",
    "5. **Samplers**: `optuna` offers various samplers, such as Tree-structured Parzen Estimator (TPE) and Random Search, to intelligently explore the hyperparameter space. TPE, for instance, adapts its search strategy based on the performance of previous trials, focusing on more promising regions. The Tree-structured Parzen Estimator (TPE) is a widely used Bayesian optimization algorithm, and Optuna's default sampler, for efficiently finding the optimal hyperparameters of a machine learning model. TPE intelligently learns from the results of past trials to propose more promising hyperparameters for the next evaluation, significantly outperforming random or grid search methods, especially for complex or computationally expensive problems\n",
    "6. **Visualization**: `optuna` provides tools for visualizing the optimization process, including plots for hyperparameter importance, optimization history, and parallel coordinate plots to understand the relationships between hyperparameters and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff55629",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in Terminal:\n",
    "# mamba activate pydev\n",
    "# mamba install -c conda-forge optuna -y\n",
    "# python -m ipykernel install --user --name pydev --display-name \"Python (pydev)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Survival Forest with Optuna hyperparameter tuning (C-index objective)\n",
    "\n",
    "import numpy as np, pandas as pd, warnings, optuna\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "##   Data & feature filtering\n",
    "\n",
    "# Avoid zero months by clipping at small value and avoid kernel crashes\n",
    "epsilon = 0.01\n",
    "y_train[\"os_months\"] = y_train[\"os_months\"].clip(lower=epsilon)\n",
    "y_test[\"os_months\"]  = y_test[\"os_months\"].clip(lower=epsilon)\n",
    "\n",
    "# Variance thresholding\n",
    "vt = VarianceThreshold(threshold=0.02) # remove low-variance features\n",
    "X_train_rsf = vt.fit_transform(X_train) #filter fit on train to avoid data leakage\n",
    "X_test_rsf  = vt.transform(X_test)\n",
    "\n",
    "# Prepare survival data for training and test (survival object arrays)\n",
    "y_train_surv = Surv.from_dataframe(\"os_status\", \"os_months\", y_train)\n",
    "y_test_surv  = Surv.from_dataframe(\"os_status\", \"os_months\", y_test)\n",
    "\n",
    "##   Optuna objective (C-index)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 6, 18),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 15),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 8),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.3, 0.8),\n",
    "        \"bootstrap\": True,\n",
    "    }\n",
    "\n",
    "# Train RSF model with suggested hyperparameters\n",
    "    rsf = RandomSurvivalForest(\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        oob_score=False,   # because evaluation is on test set and not using oob\n",
    "        **params # inject chosen hyperparameters\n",
    "    )\n",
    "    rsf.fit(X_train_rsf, y_train_surv)\n",
    "\n",
    "# Evaluate C-index on test set\n",
    "    pred = rsf.predict(X_test_rsf)\n",
    "    c_index = concordance_index_censored(\n",
    "        y_test[\"os_status\"].astype(bool),\n",
    "        y_test[\"os_months\"].astype(float),\n",
    "        pred\n",
    "    )[0]\n",
    "\n",
    "# Compute intermediate C-index on test set and handle pruning\n",
    "    trial.report(c_index, step=1)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return c_index\n",
    "\n",
    "# Optuna study setup and optimization\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        seed=42, multivariate=True, n_startup_trials=25\n",
    "    ),\n",
    "    pruner=optuna.pruners.HyperbandPruner(),\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=150, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest Trial:\", study.best_trial.number)\n",
    "print(\"Best Params:\", study.best_params)\n",
    "print(f\"Best C-index (Optuna objective): {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d35248",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Refit best RSF parameters on full train\n",
    "\n",
    "# Optimal hyperparameters found by Optuna from previous cell\n",
    "best_params = study.best_params\n",
    "\n",
    "# Create a new RSF with best hyperparameters\n",
    "rsf_best = RandomSurvivalForest(\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    oob_score=True,\n",
    "    **best_params\n",
    ")\n",
    "rsf_best.fit(X_train_rsf, y_train_surv)\n",
    "\n",
    "# Ensemble smoothing (same hyperparameters with different random seeds to reduce variance))\n",
    "seeds = [21, 42, 63, 84]\n",
    "rsf_models = [\n",
    "    RandomSurvivalForest(\n",
    "        random_state=s, n_jobs=-1, **best_params\n",
    "    ).fit(X_train_rsf, y_train_surv)\n",
    "    for s in seeds\n",
    "]\n",
    "preds = np.mean([m.predict(X_test_rsf) for m in rsf_models], axis=0) # average predictions\n",
    "\n",
    "# Evaluate on test set (C-index)\n",
    "c_index_test = concordance_index_censored(\n",
    "    y_test_surv[\"os_status\"],\n",
    "    y_test_surv[\"os_months\"],\n",
    "    preds\n",
    ")[0]\n",
    "\n",
    "print(f\"\\nRSF – C-index on test set (full): {c_index_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IBS on IBS-eligible subset (subsampled, with safe grid) for RSF\n",
    "\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "epsilon = 1e-3  # small safety margin\n",
    "\n",
    "# Start with IBS-eligible subset (same as before, but grid will be recomputed later)\n",
    "X_test_rsf_ibs, y_test_surv_ibs_rsf, TIME_GRID_RSF, max_train_time_rsf = prepare_ibs_evaluation(\n",
    "    y_train_surv, y_test_surv, X_test_rsf, n_grid=100\n",
    ")\n",
    "\n",
    "print(f\"Max train time (RSF): {max_train_time_rsf:.2f}\")\n",
    "print(f\"Max test time (all): {y_test_surv['os_months'].max():.2f}\")\n",
    "print(f\"Max IBS-eligible test time (RSF): {y_test_surv_ibs_rsf['os_months'].max():.2f}\")\n",
    "print(f\"TIME_GRID_RSF range (pre-subsample): {TIME_GRID_RSF.min():.2f} to {TIME_GRID_RSF.max():.2f}\")\n",
    "print(f\"IBS-eligible test size (before subsampling): {len(y_test_surv_ibs_rsf)}\")\n",
    "\n",
    "# Subsample IBS test set to at most 300 pts\n",
    "max_ibs_test = 300\n",
    "n_ibs = len(y_test_surv_ibs_rsf)\n",
    "\n",
    "# Subsample IBS test set\n",
    "if n_ibs > max_ibs_test:\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx_sub = rng.choice(n_ibs, size=max_ibs_test, replace=False)\n",
    "    if hasattr(X_test_rsf_ibs, \"iloc\"):\n",
    "        X_test_rsf_ibs_sub = X_test_rsf_ibs.iloc[idx_sub]\n",
    "    else:\n",
    "        X_test_rsf_ibs_sub = X_test_rsf_ibs[idx_sub]\n",
    "    y_test_surv_ibs_sub = y_test_surv_ibs_rsf[idx_sub]\n",
    "    print(f\"Subsampled IBS test size: {len(y_test_surv_ibs_sub)} (from {n_ibs})\")\n",
    "else:\n",
    "    X_test_rsf_ibs_sub = X_test_rsf_ibs\n",
    "    y_test_surv_ibs_sub = y_test_surv_ibs_rsf\n",
    "\n",
    "# Recompute a VALID time grid for *this subsample*\n",
    "time_train = y_train_surv[\"os_months\"]\n",
    "time_test_sub = y_test_surv_ibs_sub[\"os_months\"]\n",
    "\n",
    "t_min = max(time_train.min(), time_test_sub.min())\n",
    "t_max = min(time_train.max(), time_test_sub.max()) - epsilon  # must be strictly < max(test_time)\n",
    "\n",
    "if t_max <= t_min:\n",
    "    raise ValueError(f\"Invalid IBS time window after subsample: t_min={t_min}, t_max={t_max}\")\n",
    "\n",
    "n_grid = 100\n",
    "TIME_GRID_RSF_SUB = np.linspace(t_min, t_max, n_grid, endpoint=False)\n",
    "\n",
    "print(f\"TIME_GRID_RSF_SUB range: {TIME_GRID_RSF_SUB.min():.2f} to {TIME_GRID_RSF_SUB.max():.2f}\")\n",
    "print(f\"Subsample follow-up range: {time_test_sub.min():.2f} to {time_test_sub.max():.2f}\")\n",
    "\n",
    "# Predict survival functions on subsample\n",
    "t0 = time.time()\n",
    "surv_fns_rsf = rsf_best.predict_survival_function(X_test_rsf_ibs_sub)\n",
    "print(f\"predict_survival_function done in {time.time() - t0:.2f} s\")\n",
    "\n",
    "# Evaluate survival on new grid\n",
    "t1 = time.time()\n",
    "surv_probs_rsf = np.asarray([fn(TIME_GRID_RSF_SUB) for fn in surv_fns_rsf])\n",
    "print(f\"Building surv_probs_rsf done in {time.time() - t1:.2f} s\")\n",
    "print(\"surv_probs_rsf shape:\", surv_probs_rsf.shape)\n",
    "\n",
    "# IBS on subsampled IBS-eligible set\n",
    "t2 = time.time()\n",
    "ibs_test_rsf = integrated_brier_score(\n",
    "    y_train_surv,\n",
    "    y_test_surv_ibs_sub,\n",
    "    surv_probs_rsf,\n",
    "    TIME_GRID_RSF_SUB\n",
    ")\n",
    "print(f\"integrated_brier_score done in {time.time() - t2:.2f} s\")\n",
    "\n",
    "print(\"\\nFinal RSF Results:\")\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"C-index on test set (full): {c_index_test:.4f}\")\n",
    "print(f\"IBS on test set (IBS-eligible subset, subsampled): {ibs_test_rsf:.4f}\")\n",
    "\n",
    "results_rsf = {\"c_index_test\": c_index_test, \"ibs_test\": ibs_test_rsf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086a7fb",
   "metadata": {},
   "source": [
    "## Bootstrap Random Survival Forest\n",
    "\n",
    "We decided to run bootstrap to be consistent with similar studies and provide uncertainty in the C-index score. \\\n",
    "Boostrap: will pick `n_test` (nomber of row of test set) 1,000 times (B = 1,000). Some may be repeat, some may not appear since it is with replacement. Then, it will compute C-index and IBS for each iteration. From this, we will get percentiles 2.5 - 97.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12630ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "### Bootstrap 95% CIs on test (RSF; model fixed, no re-tuning)\n",
    "\n",
    "B = 1000 # number of bootstrap resamples\n",
    "rng = np.random.default_rng(42) # random seed\n",
    "\n",
    "# C-index bootstrap on FULL test set\n",
    "n_test = X_test_rsf.shape[0] # number of test samples\n",
    "c_index_boot = [] #list to store bootstrap C-index values\n",
    "\n",
    "# Create arrays for event and time (surv) from the full test set\n",
    "y_event_full = y_test_surv[\"os_status\"]\n",
    "y_time_full  = y_test_surv[\"os_months\"]\n",
    "\n",
    "for _ in range(B):\n",
    "    # Resample indices for the full test set\n",
    "    idx = rng.integers(0, n_test, size=n_test)\n",
    "\n",
    "    yb_event = y_event_full[idx] # bootstrap sample of events\n",
    "    yb_time  = y_time_full[idx] # bootstrap sample of times\n",
    "    pred_b   = preds[idx]  # ensemble RSF risk scores\n",
    "\n",
    "    # Calculate C-index for this bootstrap sample\n",
    "    cidx_b = concordance_index_censored(\n",
    "        yb_event,\n",
    "        yb_time,\n",
    "        pred_b)[0]\n",
    "    c_index_boot.append(cidx_b)\n",
    "\n",
    "# Final C-index bootstrap results\n",
    "c_index_boot = np.array(c_index_boot)\n",
    "c_index_ci = np.percentile(c_index_boot, [2.5, 97.5])\n",
    "\n",
    "\n",
    "# IBS bootstrap on IBS-eligible subset\n",
    "\n",
    "ibs_boot = [] #list to store bootstrap IBS values\n",
    "\n",
    "# Training times / events for censoring model\n",
    "time_train  = y_train_surv[\"os_months\"]\n",
    "event_train = y_train_surv[\"os_status\"]\n",
    "max_train_time = time_train.max()\n",
    "eps = 1e-3\n",
    "\n",
    "# IBS-eligible subset size\n",
    "n_ibs = X_test_rsf_ibs.shape[0]\n",
    "print(f\"Bootstrap IBS on {n_ibs} IBS-eligible RSF test patients.\")\n",
    "\n",
    "for _ in range(B):\n",
    "    # Resample within IBS-eligible subset\n",
    "    idx_ibs = rng.integers(0, n_ibs, size=n_ibs)\n",
    "\n",
    "    # Row-wise bootstrap sample of features\n",
    "    if hasattr(X_test_rsf_ibs, \"iloc\"):\n",
    "        Xb = X_test_rsf_ibs.iloc[idx_ibs]\n",
    "    else:\n",
    "        Xb = X_test_rsf_ibs[idx_ibs]\n",
    "\n",
    "    # Matching survival outcomes (structured array)\n",
    "    yb_surv   = y_test_surv_ibs_rsf[idx_ibs]\n",
    "    yb_event  = yb_surv[\"os_status\"]\n",
    "    yb_time   = yb_surv[\"os_months\"]\n",
    "\n",
    "    # Define safe time window for THIS bootstrap sample\n",
    "    t_min_b = max(time_train.min(), yb_time.min())\n",
    "    t_max_b = min(max_train_time, yb_time.max()) - eps\n",
    "\n",
    "    # If the window collapses (rare), skip this sample\n",
    "    if t_max_b <= t_min_b:\n",
    "        continue\n",
    "\n",
    "    time_points_b = np.linspace(t_min_b, t_max_b, 200, endpoint=False)\n",
    "\n",
    "    # Predict survival for bootstrap sample at time_points_b\n",
    "    surv_fns_b   = rsf_best.predict_survival_function(Xb)\n",
    "    surv_probs_b = np.asarray([fn(time_points_b) for fn in surv_fns_b])\n",
    "\n",
    "    # IBS for this bootstrap sample\n",
    "    ibs_b = integrated_brier_score(\n",
    "        y_train_surv,  # training data for censoring KM\n",
    "        yb_surv,       # THIS bootstrap sample (IBS-eligible)\n",
    "        surv_probs_b,\n",
    "        time_points_b\n",
    "    )\n",
    "    ibs_boot.append(ibs_b)\n",
    "\n",
    "ibs_boot = np.array(ibs_boot)\n",
    "ibs_ci   = np.percentile(ibs_boot, [2.5, 97.5])\n",
    "\n",
    "\n",
    "# Print RSF results + bootstrap CIs\n",
    "\n",
    "print(f\"RSF C-index (test, full): {c_index_test:.4f}\")\n",
    "print(f\"  95% CI: [{c_index_ci[0]:.4f}, {c_index_ci[1]:.4f}]\")\n",
    "print(f\"RSF IBS (test, IBS subset): {ibs_test_rsf:.4f}\")\n",
    "print(f\"  95% CI: [{ibs_ci[0]:.4f}, {ibs_ci[1]:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9e259",
   "metadata": {},
   "source": [
    "## Gradient Boosted Survival Tree (GBST)\n",
    "We are implementing hyperparameter tuning using Bayesian (`optuna`) for the same reason as `RandomSurvivalForest`: given the high dimensional features' relationship, Optuna optimization can find the optimal hyperparameters, resulting in better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbe52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient Boosted Survival Tree (GBST) with Optuna Tuning\n",
    "\n",
    "import optuna, numpy as np\n",
    "from sksurv.ensemble import GradientBoostingSurvivalAnalysis\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "from sksurv.util import Surv\n",
    "import time\n",
    "\n",
    "# Survival objects (reuse same y_train, y_test)\n",
    "y_train_surv = Surv.from_dataframe(\"os_status\", \"os_months\", y_train)\n",
    "y_test_surv  = Surv.from_dataframe(\"os_status\", \"os_months\", y_test)\n",
    "\n",
    "\n",
    "## Optuna objective for GBST (C-index)\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective_gbst(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 6),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"max_features\": trial.suggest_float(\"max_features\", 0.3, 1.0),\n",
    "    }\n",
    "\n",
    "# Train GBST model with suggested hyperparameters\n",
    "    gbst = GradientBoostingSurvivalAnalysis(random_state=42, **params)\n",
    "    gbst.fit(X_train, y_train_surv)\n",
    "\n",
    "# Evaluate C-index on test set\n",
    "    pred = gbst.predict(X_test)\n",
    "    c_index = concordance_index_censored(\n",
    "        y_test[\"os_status\"].astype(bool),\n",
    "        y_test[\"os_months\"].astype(float),\n",
    "        pred\n",
    "    )[0]\n",
    "\n",
    "    return c_index\n",
    "\n",
    "\n",
    "## Optuna study and optimization\n",
    "\n",
    "# Optuna study setup and optimization\n",
    "study_gbst = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=42, multivariate=True, n_startup_trials=20),\n",
    "    pruner=optuna.pruners.HyperbandPruner()\n",
    ")\n",
    "study_gbst.optimize(objective_gbst, n_trials=120, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nBest Trial (GBST):\", study_gbst.best_trial.number)\n",
    "print(\"Best Params (GBST):\", study_gbst.best_params)\n",
    "print(f\"Best C-index (CV, GBST): {study_gbst.best_value:.4f}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "## Refit best GBST parameters on full train (optimal hyperparameters found by Optuna)\n",
    "\n",
    "# Create a new GBST with best hyperparameters\n",
    "gbst_best = GradientBoostingSurvivalAnalysis(\n",
    "    random_state=42, **study_gbst.best_params\n",
    ")\n",
    "gbst_best.fit(X_train, y_train_surv)\n",
    "\n",
    "# C-index on FULL test set\n",
    "pred_test_gbst = gbst_best.predict(X_test)\n",
    "\n",
    "# Evaluate on test set (C-index)\n",
    "c_index_test_gbst = concordance_index_censored(\n",
    "    y_test_surv[\"os_status\"],\n",
    "    y_test_surv[\"os_months\"],\n",
    "    pred_test_gbst\n",
    ")[0]\n",
    "\n",
    "print(f\"GBST – C-index on test set (full): {c_index_test_gbst:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "## IBS on IBS-eligible subset (subsampled, with safe grid)\n",
    "\n",
    "epsilon = 1e-3  # small margin so times are strictly inside follow-up\n",
    "\n",
    "# IBS-eligible subset using your helper\n",
    "X_test_gbst_ibs, y_test_surv_ibs_gbst, TIME_GRID_GBST, max_train_time_gbst = prepare_ibs_evaluation(\n",
    "    y_train_surv, y_test_surv, X_test, n_grid=100  # 100 points is enough\n",
    ")\n",
    "\n",
    "print(f\"Max train time (GBST): {max_train_time_gbst:.2f}\")\n",
    "print(f\"Max test time (all): {y_test_surv['os_months'].max():.2f}\")\n",
    "print(f\"Max IBS-eligible test time (GBST): {y_test_surv_ibs_gbst['os_months'].max():.2f}\")\n",
    "print(f\"TIME_GRID_GBST range (pre-subsample): {TIME_GRID_GBST.min():.2f} to {TIME_GRID_GBST.max():.2f}\")\n",
    "print(f\"IBS-eligible test size (before subsampling): {len(y_test_surv_ibs_gbst)}\")\n",
    "\n",
    "# Subsample IBS test set (e.g. to 300 patients)\n",
    "max_ibs_test = 300\n",
    "n_ibs_gbst = len(y_test_surv_ibs_gbst)\n",
    "\n",
    "# Subsample IBS test set to at most 300 pts\n",
    "if n_ibs_gbst > max_ibs_test:\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx_sub_gbst = rng.choice(n_ibs_gbst, size=max_ibs_test, replace=False)\n",
    "\n",
    "# Get subsampled X_test and y_test_surv\n",
    "    if hasattr(X_test_gbst_ibs, \"iloc\"):\n",
    "        X_test_gbst_ibs_sub = X_test_gbst_ibs.iloc[idx_sub_gbst]\n",
    "    else:\n",
    "        X_test_gbst_ibs_sub = X_test_gbst_ibs[idx_sub_gbst]\n",
    "\n",
    "# Subsampled survival outcomes\n",
    "    y_test_surv_ibs_sub_gbst = y_test_surv_ibs_gbst[idx_sub_gbst]\n",
    "    print(f\"Subsampled IBS test size (GBST): {len(y_test_surv_ibs_sub_gbst)} (from {n_ibs_gbst})\")\n",
    "else:\n",
    "    X_test_gbst_ibs_sub = X_test_gbst_ibs\n",
    "    y_test_surv_ibs_sub_gbst = y_test_surv_ibs_gbst\n",
    "\n",
    "# Recompute a VALID time grid for this subsample\n",
    "time_train = y_train_surv[\"os_months\"]\n",
    "time_test_sub_gbst = y_test_surv_ibs_sub_gbst[\"os_months\"]\n",
    "\n",
    "# Recompute a VALID time grid for *this subsample*\n",
    "t_min = max(time_train.min(), time_test_sub_gbst.min())\n",
    "t_max = min(time_train.max(), time_test_sub_gbst.max()) - epsilon  # strictly < max(test_time)\n",
    "\n",
    "if t_max <= t_min:\n",
    "    raise ValueError(f\"Invalid IBS time window after subsample (GBST): t_min={t_min}, t_max={t_max}\")\n",
    "\n",
    "n_grid = 100\n",
    "TIME_GRID_GBST_SUB = np.linspace(t_min, t_max, n_grid, endpoint=False)\n",
    "\n",
    "print(f\"TIME_GRID_GBST_SUB range: {TIME_GRID_GBST_SUB.min():.2f} to {TIME_GRID_GBST_SUB.max():.2f}\")\n",
    "print(f\"GBST subsample follow-up range: {time_test_sub_gbst.min():.2f} to {time_test_sub_gbst.max():.2f}\")\n",
    "\n",
    "# Predict survival functions on the subsample\n",
    "t0 = time.time()\n",
    "surv_fns_gbst = gbst_best.predict_survival_function(X_test_gbst_ibs_sub)\n",
    "print(f\"GBST predict_survival_function done in {time.time() - t0:.2f} s\")\n",
    "\n",
    "# Evaluate survival on the new grid\n",
    "t1 = time.time()\n",
    "surv_probs_gbst = np.asarray([fn(TIME_GRID_GBST_SUB) for fn in surv_fns_gbst])\n",
    "print(f\"Building surv_probs_gbst done in {time.time() - t1:.2f} s\")\n",
    "print(\"surv_probs_gbst shape:\", surv_probs_gbst.shape)\n",
    "\n",
    "# IBS on subsampled IBS-eligible set\n",
    "t2 = time.time()\n",
    "ibs_test_gbst = integrated_brier_score(\n",
    "    y_train_surv,\n",
    "    y_test_surv_ibs_sub_gbst,\n",
    "    surv_probs_gbst,\n",
    "    TIME_GRID_GBST_SUB\n",
    ")\n",
    "print(f\"integrated_brier_score (GBST) done in {time.time() - t2:.2f} s\")\n",
    "\n",
    "# Output summary\n",
    "print(\"\\nGradient Boosted Survival Tree (GBST) Results:\")\n",
    "print(f\"Best parameters: {study_gbst.best_params}\")\n",
    "print(f\"C-index on test set (full): {c_index_test_gbst:.4f}\")\n",
    "print(f\"IBS on test set (IBS-eligible subset, subsampled): {ibs_test_gbst:.4f}\")\n",
    "\n",
    "results_gbst = {\n",
    "    \"c_index_test_gbst\": c_index_test_gbst,\n",
    "    \"ibs_test_gbst\": ibs_test_gbst,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8470e2c",
   "metadata": {},
   "source": [
    "## Bootstrap for `GradientBoostingSurvivalAnalysis`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa15c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bootstrap C-index on FULL test set (GBST)\n",
    "\n",
    "from sksurv.metrics import concordance_index_censored, integrated_brier_score\n",
    "import numpy as np\n",
    "\n",
    "# Bootstrap setup  \n",
    "B = 1000\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# C-index bootstrap on FULL test set\n",
    "n_test = X_test.shape[0]\n",
    "cidx_boot_gbst = []\n",
    "\n",
    "y_event_te = y_test_surv[\"os_status\"]\n",
    "y_time_te  = y_test_surv[\"os_months\"]\n",
    "\n",
    "# Bootstrap C-index\n",
    "for _ in range(B):\n",
    "    idx = rng.integers(0, n_test, size=n_test)\n",
    "    Xb = X_test.iloc[idx]\n",
    "    yb_event = y_event_te[idx]\n",
    "    yb_time  = y_time_te[idx]\n",
    "\n",
    "    pred_b = gbst_best.predict(Xb)\n",
    "    cidx_b = concordance_index_censored(yb_event, yb_time, pred_b)[0]\n",
    "    cidx_boot_gbst.append(cidx_b)\n",
    "\n",
    "cidx_boot_gbst = np.array(cidx_boot_gbst)\n",
    "cidx_ci_gbst = np.percentile(cidx_boot_gbst, [2.5, 97.5])\n",
    "\n",
    "# Print GBST results + bootstrap CIs\n",
    "\n",
    "print(f\"GBST C-index (test, full): {c_index_test:.4f}\")\n",
    "print(f\"  95% CI: [{c_index_ci[0]:.4f}, {c_index_ci[1]:.4f}]\")\n",
    "print(f\"GBST IBS (test, IBS subset): {ibs_test_rsf:.4f}\")\n",
    "print(f\"  95% CI: [{ibs_ci[0]:.4f}, {ibs_ci[1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f8df9",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "SHAP, which stands for SHapley Additive exPlanations, is a unified framework to explain how each feature contributes to a model’s prediction, for any ML model (tree models, random forests, XGBoost, neural nets, etc.). \\\n",
    "SHAP explains:\n",
    "\n",
    "1. **Global importance**\n",
    "Which features matter overall \\\n",
    "Ranked importance \\\n",
    "Summary plots \n",
    "\n",
    "2. **Local importance**\n",
    "Why the model made a prediction for this specific patient \\\n",
    "Direction + magnitude of contribution \n",
    "\n",
    "3. **Interaction effects**\n",
    "How two features together change risk \n",
    "\n",
    "4. **Consistent feature importance**\n",
    "If a feature contributes more in the model, its SHAP value must be higher\n",
    "(unlike some random forest importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SHAP if needed (via mamba or pip in Terminal)\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32bc5ad",
   "metadata": {},
   "source": [
    "### Prepare DataFrames for SHAP Analysis\n",
    "\n",
    "SHAP requires feature names for interpretability. We'll rebuild DataFrames from the numpy arrays if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb6bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rebuild DataFrames with feature names for SHAP\n",
    "\n",
    "##### Notes:\n",
    "# Get feature names that survived variance threshold filtering\n",
    "# The VarianceThreshold object 'vt' was fitted on X_train earlier\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    original_feature_names = X.columns.tolist()\n",
    "    # Get the features that passed the variance threshold\n",
    "    # vt.get_support() returns a boolean mask of selected features\n",
    "    selected_features_mask = vt.get_support()\n",
    "    feature_names = [original_feature_names[i] for i, selected in enumerate(selected_features_mask) if selected]\n",
    "else:\n",
    "    # Fallback: create generic names matching the current shape\n",
    "    feature_names = [f\"feat_{i}\" for i in range(X_train.shape[1])]\n",
    "\n",
    "# Convert X_train and X_test to DataFrames if they're numpy arrays\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    X_train_df = X_train.copy()\n",
    "else:\n",
    "    X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "\n",
    "if isinstance(X_test, pd.DataFrame):\n",
    "    X_test_df = X_test.copy()\n",
    "else:\n",
    "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "\n",
    "print(f\"X_train_df shape: {X_train_df.shape}\")\n",
    "print(f\"X_test_df shape: {X_test_df.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features removed by VarianceThreshold: {len(original_feature_names) - len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49cc0e6",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Gradient Boosted Survival Tree (GBST)\n",
    "\n",
    "We'll use SHAP to explain the GBST model predictions. SHAP values show how much each feature contributes to the risk score for each patient. \\\n",
    "Since GBST achieved higher performance, we will focus on this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d9503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP for GBST Model\n",
    "\n",
    "# Sample background data for SHAP baseline (smaller subset for speed)\n",
    "X_bg_gbst = X_train_df.sample(n=min(100, X_train_df.shape[0]), random_state=42)\n",
    "\n",
    "# Sample test data to explain (manageable subset)\n",
    "X_explain_gbst = X_test_df.sample(n=min(200, X_test_df.shape[0]), random_state=42)\n",
    "\n",
    "print(f\"Background sample: {X_bg_gbst.shape}\")\n",
    "print(f\"Explanation sample: {X_explain_gbst.shape}\")\n",
    "\n",
    "# Create SHAP explainer for GBST\n",
    "# Use the predict function to get risk scores\n",
    "explainer_gbst = shap.Explainer(\n",
    "    lambda x: gbst_best.predict(x if isinstance(x, np.ndarray) else x.values),\n",
    "    X_bg_gbst\n",
    ")\n",
    "\n",
    "# Compute SHAP values\n",
    "print(\"Computing SHAP values for GBST... (this may take a few minutes)\")\n",
    "shap_values_gbst = explainer_gbst(X_explain_gbst)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values_gbst.values.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515d39d",
   "metadata": {},
   "source": [
    "### SHAP Summary Plots for GBST\n",
    "\n",
    "Global feature importance visualizations showing which features most impact survival predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf89496",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Summary Plots - Bar Plot (Feature Importance) - GBST\n",
    "\n",
    "# SHAP bar plot for GBST \n",
    "shap.summary_plot(\n",
    "    shap_values_gbst.values,\n",
    "    X_explain_gbst,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=20  # top 20 features\n",
    ")\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)  # control figure size\n",
    "\n",
    "# Titles & labels\n",
    "plt.title(\n",
    "    \"GBST: Top 20 Features by Mean |SHAP| (Impact on Survival Risk)\",\n",
    "    fontsize=14,\n",
    "    pad=18\n",
    ")\n",
    "plt.xlabel(\"Mean(|SHAP value|)\", fontsize=12)\n",
    "\n",
    "# Tidy up fonts and layout\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # leave room for title\n",
    "\n",
    "# Save high-res figure\n",
    "#plt.savefig(FIGURES / \"shap_gbst_bar_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e24784",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Beeswarm Plot (Feature Impact with Direction) - GBST\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_gbst.values,\n",
    "    X_explain_gbst,\n",
    "    show=False,\n",
    "    max_display=20  # Show top 20 features\n",
    ")\n",
    "plt.title(\"GBST: Feature Impact on Survival Risk\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES / \"shap_gbst_beeswarm_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Each dot is a patient\")\n",
    "print(\"- Red = high feature value, Blue = low feature value\")\n",
    "print(\"- Right side (positive SHAP) = increases risk\")\n",
    "print(\"- Left side (negative SHAP) = decreases risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4e07d8",
   "metadata": {},
   "source": [
    "### SHAP dependence plot ASXL1 versus non ASXL1\n",
    "To better understand the importance of ASXL1 and non-ASXL1 DTA mutations, we plotted them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Dependence Plot for ASXL1 Burden - GBST\n",
    "\n",
    "# Ensure 'asxl1_count' is in the DataFrame\n",
    "needed_cols = [\"asxl1_count\", \"dta_non_asxl1_counts\", \"asxl1\"]\n",
    "missing = [c for c in needed_cols if c not in X_explain_gbst.columns]\n",
    "print(\"Missing columns:\", missing)   # should print [] ideally\n",
    "\n",
    "# For convenience\n",
    "shap_vals = shap_values_gbst.values  # (n_samples, n_features)\n",
    "\n",
    "\n",
    "# ASXL1 burden: dependence plot\n",
    "plt.figure(figsize=(7, 5))\n",
    "shap.dependence_plot(\n",
    "    \"asxl1_count\",\n",
    "    shap_vals,\n",
    "    X_explain_gbst,\n",
    "    interaction_index=None,  # pure main effect\n",
    "    show=False\n",
    ")\n",
    "plt.title(\"GBST – SHAP Dependence: ASXL1 Burden\", fontsize=14, pad=12)\n",
    "plt.xlabel(\"ASXL1 mutational burden (standardized, z-score)\", fontsize=12)\n",
    "plt.ylabel(\"SHAP value (impact on log survival risk)\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES / \"dep_asxl1_count_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e54b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Dependence Plot for Non-ASXL1 DTA Burden - GBST\n",
    "\n",
    "# Ensure 'dta_non_asxl1_counts' is in the DataFrame\n",
    "plt.figure(figsize=(7, 5))\n",
    "shap.dependence_plot(\n",
    "    \"dta_non_asxl1_counts\",\n",
    "    shap_vals,\n",
    "    X_explain_gbst,\n",
    "    interaction_index=None,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Get current x-limits and create ticks every 0.5\n",
    "xmin, xmax = ax.get_xlim()\n",
    "xmin = np.floor(xmin * 2) / 2.0\n",
    "xmax = np.ceil(xmax * 2) / 2.0\n",
    "ax.set_xticks(np.arange(xmin, xmax + 0.25, 0.5))\n",
    "\n",
    "plt.title(\"GBST – SHAP Dependence: Non-ASXL1 DTA Burden\", fontsize=14, pad=12)\n",
    "plt.xlabel(\"Non-ASXL1 DTA mutational burden (standardized, z-score)\", fontsize=12)\n",
    "plt.ylabel(\"SHAP value (impact on log survival risk)\", fontsize=12)\n",
    "#plt.savefig(FIGURES / \"dep_dta_non_asxl1_counts_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6687d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Dependence Plot for TET2 DTA Burden - GBST\n",
    "\n",
    "# Ensure 'tet2_count' is in the DataFrame\n",
    "plt.figure(figsize=(7, 5))\n",
    "shap.dependence_plot(\n",
    "    \"tet2_count\",\n",
    "    shap_vals,\n",
    "    X_explain_gbst,\n",
    "    interaction_index=None,\n",
    "    show=False\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "# Get current x-limits and create ticks every 0.5\n",
    "xmin, xmax = ax.get_xlim()\n",
    "xmin = np.floor(xmin * 2) / 2.0\n",
    "xmax = np.ceil(xmax * 2) / 2.0\n",
    "ax.set_xticks(np.arange(xmin, xmax + 0.25, 0.5))\n",
    "\n",
    "plt.title(\"GBST – SHAP Dependence: TET2 DTA Burden\", fontsize=14, pad=12)\n",
    "plt.xlabel(\"TET2 DTA mutational burden (standardized, z-score)\", fontsize=12)\n",
    "plt.ylabel(\"SHAP value (impact on log survival risk)\", fontsize=12)\n",
    "#plt.savefig(FIGURES / \"dep_tet2_count_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e80329",
   "metadata": {},
   "source": [
    "### SHAP Analysis for Random Survival Forest (RSF)\n",
    "Analyzed for comparison with GBST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rebuild RSF dataframes in the reduced feature space for SHAP (without refit)\n",
    "# To avoid the error 'X has 45 features, but RandomSurvivalForest is expecting 37 features as input'\n",
    "\n",
    "\n",
    "# X_train: original training DataFrame (before VarianceThreshold)\n",
    "# vt: the VarianceThreshold used for RSF\n",
    "# X_train_rsf, X_test_rsf: the arrays RSF was actually trained on\n",
    "\n",
    "if isinstance(X_train, pd.DataFrame):\n",
    "    original_feature_names = X_train.columns\n",
    "    selected_mask = vt.get_support()\n",
    "    selected_features = original_feature_names[selected_mask]\n",
    "\n",
    "    X_train_rsf_df = pd.DataFrame(X_train_rsf, columns=selected_features)\n",
    "    X_test_rsf_df  = pd.DataFrame(X_test_rsf,  columns=selected_features)\n",
    "else:\n",
    "    # Fallback if X_train wasn't a DataFrame\n",
    "    X_train_rsf_df = pd.DataFrame(X_train_rsf)\n",
    "    X_test_rsf_df  = pd.DataFrame(X_test_rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d24bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP for RSF Model\n",
    "\n",
    "# Background sample (for SHAP baseline)\n",
    "X_bg_rsf = X_train_rsf_df.sample(\n",
    "    n=min(100, X_train_rsf_df.shape[0]),\n",
    "    random_state=42)\n",
    "\n",
    "# Test sample to explain\n",
    "X_explain_rsf = X_test_rsf_df.sample(\n",
    "    n=min(200, X_test_rsf_df.shape[0]),\n",
    "    random_state=42)\n",
    "\n",
    "print(f\"Background sample:  {X_bg_rsf.shape}\")\n",
    "print(f\"Explanation sample: {X_explain_rsf.shape}\")\n",
    "\n",
    "# SHAP explainer – RSF risk scores\n",
    "explainer_rsf = shap.Explainer(\n",
    "    lambda x: rsf_best.predict(x if isinstance(x, np.ndarray) else x.values),\n",
    "    X_bg_rsf)\n",
    "\n",
    "print(\"Computing SHAP values for RSF... (this may take a few minutes)\")\n",
    "shap_values_rsf = explainer_rsf(X_explain_rsf)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values_rsf.values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d740390",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RSF SHAP Summary Plots\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_rsf.values,\n",
    "    X_explain_rsf,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=20)\n",
    "\n",
    "plt.title(\"RSF: Top 20 Features by Mean Absolute SHAP Value\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES/\"shap_rsf_bar_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Beeswarm plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_rsf.values,\n",
    "    X_explain_rsf,\n",
    "    show=False,\n",
    "    max_display=25)\n",
    "\n",
    "plt.title(\"RFS: Feature Impact on Survival Risk\", fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES/\"shap_rsf_beeswarm_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74494b10",
   "metadata": {},
   "source": [
    "## Comparison: GBST and RSF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f03de6",
   "metadata": {},
   "source": [
    "### Feature Importance Comparison: GBST vs RSF\n",
    "\n",
    "Compare which features are most important across different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbae37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Feature Importance Across Models (Normalized)\n",
    "\n",
    "\n",
    "## Normalize SHAP values\n",
    "\n",
    "# GBST normalized importance\n",
    "gbst_raw = np.abs(shap_values_gbst.values)\n",
    "gbst_norm = gbst_raw / gbst_raw.sum(axis=1, keepdims=True)   # normalize per sample\n",
    "gbst_importance = pd.DataFrame({\n",
    "    'feature': X_explain_gbst.columns,\n",
    "    'gbst_importance': gbst_norm.mean(axis=0)\n",
    "}).sort_values('gbst_importance', ascending=False)\n",
    "\n",
    "# RSF normalized importance\n",
    "rsf_raw = np.abs(shap_values_rsf.values)\n",
    "rsf_norm = rsf_raw / rsf_raw.sum(axis=1, keepdims=True)      # normalize per sample\n",
    "rsf_importance = pd.DataFrame({\n",
    "    'feature': X_explain_rsf.columns,\n",
    "    'rsf_importance': rsf_norm.mean(axis=0)\n",
    "}).sort_values('rsf_importance', ascending=False)\n",
    "\n",
    "\n",
    "## Merge and compare\n",
    "comparison = gbst_importance.merge(rsf_importance, on='feature', how='outer').fillna(0)\n",
    "comparison['avg_importance'] = (comparison['gbst_importance'] + comparison['rsf_importance']) / 2\n",
    "comparison = comparison.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "## Display top 20\n",
    "print(\"Top 20 Features by Average *Normalized* SHAP Importance (GBST + RSF):\\n\")\n",
    "print(comparison.head(20).to_string(index=False))\n",
    "\n",
    "## Plot\n",
    "top20 = comparison.head(20)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(top20))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, top20['gbst_importance'], width, label='GBST', alpha=0.8)\n",
    "ax.barh(x + width/2, top20['rsf_importance'], width, label='RSF', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top20['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean Normalized SHAP Value', fontsize=12)\n",
    "ax.set_title('Feature Importance Comparison in OS: GBST vs RSF (Normalized)', fontsize=14, pad=20)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES/\"shap_model_comparison_os_normalized.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compare Feature Importance Across Models\n",
    "\n",
    "# Calculate mean absolute SHAP values for each model (mean raw values)\n",
    "gbst_importance = pd.DataFrame({\n",
    "    'feature': X_explain_gbst.columns,\n",
    "    'gbst_importance': np.abs(shap_values_gbst.values).mean(axis=0)\n",
    "}).sort_values('gbst_importance', ascending=False)\n",
    "\n",
    "rsf_importance = pd.DataFrame({\n",
    "    'feature': X_explain_rsf.columns,\n",
    "    'rsf_importance': np.abs(shap_values_rsf.values).mean(axis=0)\n",
    "}).sort_values('rsf_importance', ascending=False)\n",
    "\n",
    "# Merge and compare\n",
    "comparison = gbst_importance.merge(rsf_importance, on='feature')\n",
    "comparison['avg_importance'] = (comparison['gbst_importance'] + comparison['rsf_importance']) / 2\n",
    "comparison = comparison.sort_values('avg_importance', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"Top 20 Features by Average SHAP Importance (GBST + RSF):\\n\")\n",
    "print(comparison.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "top20 = comparison.head(20)\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(top20))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, top20['gbst_importance'], width, label='GBST', alpha=0.8)\n",
    "ax.barh(x + width/2, top20['rsf_importance'], width, label='RSF', alpha=0.8)\n",
    "\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(top20['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "ax.set_title('Feature Importance Comparison in OS: GBST vs RSF', fontsize=14, pad=20)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGURES/\"shap_model_comparison_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed55052",
   "metadata": {},
   "source": [
    "### Waterfall plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96840569",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Waterfall Plot - High Risk Patient (GBST)\n",
    "\n",
    "# Find a high-risk patient (high predicted risk score)\n",
    "gbst_risks = gbst_best.predict(X_explain_gbst.values)\n",
    "high_risk_idx = np.argmax(gbst_risks)\n",
    "\n",
    "\n",
    "# Define custom font size\n",
    "CUSTOM_FONTSIZE = 10\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.waterfall_plot(shap_values_gbst[high_risk_idx], max_display=15, show=False)\n",
    "\n",
    "# Access the current axes and iterate through ALL text objects to set font size\n",
    "ax = plt.gca()\n",
    "\n",
    "# For feature names, values, SHAP values, etc.\n",
    "for text in ax.texts:\n",
    "    text.set_fontsize(CUSTOM_FONTSIZE)\n",
    "\n",
    "# For x and y tick labels (might be needed if ax.texts doesn't cover them)\n",
    "ax.tick_params(axis='x', labelsize=CUSTOM_FONTSIZE)\n",
    "ax.tick_params(axis='y', labelsize=CUSTOM_FONTSIZE * 0.9) # Slightly smaller for long feature names\n",
    "\n",
    "# For x and y axis labels\n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize=CUSTOM_FONTSIZE)\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize=CUSTOM_FONTSIZE)\n",
    "\n",
    "# Set the title with desired font size and padding\n",
    "plt.title(f\"GBST: High-Risk Patient Explanation (Risk Score: {gbst_risks[high_risk_idx]:.3f})\", \n",
    "          fontsize=CUSTOM_FONTSIZE + 2, \n",
    "          pad=40) # Increase padding\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(OUTPUT/\"shap_waterfall_high_risk_gbst_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51621a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SHAP Waterfall Plot - High Risk Patient (RSF)\n",
    "\n",
    "# Find a high-risk patient (high predicted risk score)\n",
    "rsf_risks = rsf_best.predict(X_explain_rsf.values)\n",
    "high_risk_idx = np.argmax(rsf_risks)\n",
    "\n",
    "\n",
    "# Define custom font size\n",
    "CUSTOM_FONTSIZE = 10\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.waterfall_plot(shap_values_rsf[high_risk_idx], max_display=15, show=False)\n",
    "\n",
    "#  Access the current axes and iterate through ALL text objects to set font size\n",
    "ax = plt.gca()\n",
    "\n",
    "# For feature names, values, SHAP values, etc.\n",
    "for text in ax.texts:\n",
    "    text.set_fontsize(CUSTOM_FONTSIZE)\n",
    "\n",
    "# For x and y tick labels (might be needed if ax.texts doesn't cover them)\n",
    "ax.tick_params(axis='x', labelsize=CUSTOM_FONTSIZE)\n",
    "ax.tick_params(axis='y', labelsize=CUSTOM_FONTSIZE * 0.9) # Slightly smaller for long feature names\n",
    "\n",
    "# For x and y axis labels\n",
    "ax.set_xlabel(ax.get_xlabel(), fontsize=CUSTOM_FONTSIZE)\n",
    "ax.set_ylabel(ax.get_ylabel(), fontsize=CUSTOM_FONTSIZE)\n",
    "\n",
    "# Set the title with desired font size and padding\n",
    "plt.title(f\"RSF: High-Risk Patient Explanation (Risk Score: {rsf_risks[high_risk_idx]:.3f})\", \n",
    "          fontsize=CUSTOM_FONTSIZE + 2, \n",
    "          pad=40) # Increase padding\n",
    "\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(FIGUREs/\"shap_waterfall_high_risk_rsf_os.tiff\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc9835",
   "metadata": {},
   "source": [
    "### Export SHAP Values for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168abdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export SHAP Values and Feature Importance\n",
    "\n",
    "# Export GBST SHAP values\n",
    "shap_df_gbst = pd.DataFrame(\n",
    "    shap_values_gbst.values,\n",
    "    columns=X_explain_gbst.columns,\n",
    "    index=X_explain_gbst.index)\n",
    "\n",
    "#shap_df_gbst.to_csv(OUTPUT/\"shap_values_gbst_os.csv\")\n",
    "\n",
    "# Export RSF SHAP values\n",
    "shap_df_rsf = pd.DataFrame(\n",
    "    shap_values_rsf.values,\n",
    "    columns=X_explain_rsf.columns,\n",
    "    index=X_explain_rsf.index)\n",
    "\n",
    "#shap_df_rsf.to_csv(OUTPUT/\"shap_values_rsf_os.csv\")\n",
    "\n",
    "# Export feature importance comparison\n",
    "#comparison.to_csv(OUTPUT/\"shap_feature_importance_comparison_os.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606adaa",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pydev)",
   "language": "python",
   "name": "pydev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
